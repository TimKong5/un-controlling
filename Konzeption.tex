% !TEX root =  master.tex
\chapter{Konzeption des Kubernetes Clusters}
\label{Konzeption_K8s_Cluster}
In den folgenden Unterkapiteln folgt die im Rahmen der vorliegenden Thesis praktische Konzeption der Portierung der \ac{SaaS}-Lösung auf ein Kubernetes Cluster. Dabei erfolgt die Auswahl der Microservices, welche für die prototypische Umsetzung verwendet werden. Zudem werden die für eine \ac{SaaS}-Lösung benötigten Mechanismen, wie unter anderem die Service-to-Service Kommunikation oder die Trennung unterschiedlicher Landschaften, konzeptioniert.

\section{Auswahl der Microservices für den Prototyp}
\label{Konzeption_Auswahl_Microservices}
Bei der Auswahl der Microservices wurde besonders auf die Repräsentativität möglichst aller Komponenten der aktuellen \ac{CF}-Lösung geachtet. Außerdem sollen mittels der ausgewählten Microservices alle eingesetzten Datenbanktypen auch auf dem Kubernetes Cluster abgebildet werden. Besonders aufgrund des Zieles der Durchführung eines repräsentativen Performancetests wurde hierfür der Rater-Microservice ausgewählt, da dieser die meiste Rechenkapazität in Anspruch nimmt. Zusätzlich eignete sich dieser zum Testen der Service-to-Service Kommunikation, da er hierfür ausschließlich eine weitere Abhängigkeit zum Business-Config-Microservice benötigt. Des Weiteren existieren für den Rater-Microservice umfangreiche Mock-Daten und Integrationstests, welche zum Testen der Funktionalität des Prototyps verwendet werden können.\\
Aus fachlicher Sicht werden mit Hilfe des Business-Config-Services alle unternehmensinternen Konfigurationen vorgenommen. Diese sind unter anderem die zur Verfügung stehenden Bewertungsmetriken, welche beispielsweise eine nutzungsabhängige Bewertung in der Einheit \ac{GB} oder auch eine monatlich wiederkehrende Gebühr in der hinterlegten Währung sein können. Diese Konfigurationen verwendet der Rater-Microservice um mit den Nutzungsdaten des aktuellen Zeitraumes, wie etwa 50 \ac{GB}, die für den Kunden anfallenden Gebühren zu berechnen.\\
Für die Bereitstellung des Rater-Microservice wird eine MongoDB-Datenbank sowie die generelle RabbitMQ-Instanz benötigt. 
Beim Business-Config-Microservice hingegen wird eine PostgreSQL-Datenbank und ebenso die RabbitMQ-Instanz verwendet.\\
Die lokale Bereitstellung der zuvor genannten Datenbanken ist zudem Teil der prototypischen Portierung und soll ebenso auf dem gleichen Kubernetes Cluster durchgeführt werden. 

\section{Verschiedene Landschaften}
\label{Konzeption_Landschaften}
Generell sollten die unterschiedlichen Landschaften unabhängig und voneinander isoliert bereitgestellt werden. Deshalb sollte im Fall von SAP Subscription Billing mindestens ein separates Cluster für die Produktivumgebung und ein weiteres Cluster für die Entwicklungs-, Test- und Akzeptanzlandschaft betrieben werden. Dadurch können beispielsweise weitere Tools ohne Einfluss auf die Produktivumgebung erstmalig auf dem Entwicklungscluster implementiert und getestet werden. Hierdurch soll besonders die Performance und Verfügbarkeit der Produktivumgebung geschützt und nicht von Änderung in der Entwicklungsumgebung beeinflusst werden.
Jedoch ist hierbei zu beachten, dass der Betrieb mehrerer Cluster aufwendiger und kostenintensiver ist, als ein Konzept mit einem einzigen Kubernetes Cluster. \\
Da die Verwendung eines Multi-Cluster-Ansatzes besonders aus Kostengründen im Rahmen der vorliegenden Thesis nicht möglich ist, soll die Trennung der Umgebungen mittels der Kubernetes \textbf{Namespaces} und \textbf{Network Policies} umgesetzt werden.\\
Ein Namespace wird zur Gruppierung von Kubernetes-Objekten zu einem Gültigkeitsbereich verwendet. Dies ermöglicht die Bereitstellung gleichnamiger Objekte in unterschiedlichen Namespaces. Jedoch sollte bei der Verwendung von Namespaces beachtet werden, dass grundsätzlich keine Isolation der Pods aus unterschiedlichen Namespaces vorhanden ist. Dadurch ist beispielsweise ein Pod in der Produktivlandschaft nicht vor einem ungewollten Zugriff aus der Entwicklungslandschaft geschützt. Deshalb soll die Isolation der Pods aus unterschiedlichen Landschaften mit Hilfe von Network Policies umgesetzt werden. Eine Network Policy schränkt dabei die eingehende und ausgehende Kommunikation eines bereitgestellten Pods ein und definiert die ausschließlich erlaubten Kommunikationswege. Damit dienen Network Policies auch zur Erweiterung des Sicherheitskonzeptes, um unerwünschte Kommunikation innerhalb des Clusters zu unterbinden.\autocite[Vgl.][Network Policies
]{KubernetesAuthors.20200207}\\
Bis auf wenige Ausnahmen sind alle in der vorliegenden Thesis verwendeten Kubernetes-Objekte spezifisch dem Namespace zugeordnet und stehen somit nur innerhalb desselben Namespaces zur Verfügung. Ausnahmen sind zum Beispiel die \textbf{Persistent Volumes} oder auch die \textbf{Storage Classes}. Die Erläuterung der beiden Objekte erfolgt in Kapitel \ref{Umsetzung_Bereitstellung_Microservices}.\\
Bei der erstmaligen Konzeption des Kubernetes Clusters sollte die Auflösung einer \ac{HTTP}-Anfrage zu dem entsprechenden Namespace mit Hilfe der Kubernetes \textbf{Ingress Services} zugeordnet werden. Ein Ingress Service erstellt und verwaltet einen extern verfügbaren \ac{HTTP}-Endpunkt und routet die Anfrage abhängig vom Hostnamen oder der weiteren Pfadangabe an den im Ingress-Service hinterlegten internen Dienst.\autocite[Vgl.][What is Ingress]{KubernetesAuthors.20191018} Zudem dient der Ingress-Service zeitgleich als Load Balancer.\autocite[Vgl.][Loadbalancing]{KubernetesAuthors.20191018}
\\
Nach der erfolgreichen Umsetzung des Routings der Anfragen zur entsprechenden Umgebung mittels der Ingress Services zeigte sich jedoch der zusätzliche Wunsch der Abschaffung des aktuell verwendeten Landscape-Router-Services. Dieser ist, wie in Kapitel \ref{technischer_aufbau_cf} erläutert, ein Infrastruktur-Microservice für das mandantenabhängige Routing der externen Anfragen.
Jedoch wurde festgestellt, dass nicht alle Funktionalitäten des Landscape-Routers mittels des nativen Kubernetes Ingress-Objektes abgebildet werden können. Deshalb wurde die Verwendung der umfangreicheren Mechanismen für das Routing anhand des zusätzlichen Service Meshes \textbf{Istio} konzeptioniert. Die Erläuterung der generellen technischen Funktionsweise von Istio findet in Kapitel \ref{Umsetzung_K8s_Cluster} statt.
\\
Für das Routing der Anfragen deklariert Istio mit Hilfe der von Kubernetes unterstützten \textbf{Custom Ressource Definitions} eigene \ac{API}-Objekte, wie beispielsweise \textbf{Virtual Services}, \textbf{Gateways} und \textbf{Destination Rules}. Durch diese zusätzlichen Objekte soll das mandantenabhängige Routing des Landscape Routers abgebildet und implementiert werden. Dabei sollen im Virtual Service Regeln für das Routing der Anfragen basierend auf den Feldern des \ac{HTTP}-Headers implementiert werden. Die einzelnen von Istio definierten Objekte werden in Kapitel \ref{Umsetzung_Landschaften} erläutert. Die Erklärung einer Custom Ressource Definition erfolgt in Kapitel \ref{bewertung_k8s_prototyp}.
\section{Integration in die \acs{CI}/\acs{CD}-Pipeline}
\label{Konzeption_integration_ci_cd_pipeline}
Die Bereitstellung der Microservices auf das Kubernetes Cluster soll durch die Integration in die \ac{CI}/\ac{CD}-Pipelines automatisiert werden. Da zum aktuellen Zeitpunkt der Thesis innerhalb des SAP Subscription Billing Projektes ein Jenkins-Server als \ac{CI}/\ac{CD}-Tool genutzt wurde, wurde vom Infrastruktur-Team eine Integration in den bereits vorhandenen Jenkins-Server und deren Pipelines gewünscht. Hierbei sollen jeweils die vorhandenen Pipelines des Rater- und des Business-Config-Microservices verwendet und um eine weitere \textbf{Stage} erweitert werden. Die zuvor genannten Ressourcen von Jenkins werden innerhalb der Vorstellung der praktischen Umsetzung in Kapitel \ref{Umsetzung_CI_CD_Integration} erläutert.
\\
Zudem soll die Bereitstellung der Microservices auf dem Kubernetes Cluster mit Hilfe des Tools \textbf{Skaffold} weiter automatisiert werden. Die hierfür benötigten Schritte der Anwendungsbereitstellung werden ebenfalls in Kapitel \ref{Umsetzung_CI_CD_Integration} erläutert.\\
Skaffold ist ein Kommandozeilen-Tool zur automatisieren und kontinuierlichen Bereitstellung einer Anwendung auf einem Kubernetes Cluster. 
Der Hauptvorteil bei der Verwendung von Skaffold ist die Optimierung und die weitere Automatisierung der einzelnen Prozesse der Bereitstellung einer Anwendung in einem Kubernetes Cluster.\autocite[Vgl.][]{SkaffoldAuthors.20200131}
\newpage
Außerdem bietet Skaffold zum Beispiel Möglichkeiten für eine \textbf{Tag Policy} der Docker-Images an. Hierbei kann beispielsweise mit der Tag Policy \textbf{dateTime} die automatische Kennzeichnung des Docker-Images mittels des aktuellen Zeitstempels festgelegt werden.\autocite[Vgl.][]{SkaffoldAuthors.20200131b} \\
Innerhalb des Prototyps sollen die Docker-Images jeweils mit der zuvor erläuterten Tag Policy gekennzeichnet werden. Für die Bereitstellung der Microservices soll jeweils immer die aktuellste Version des Docker-Images aus der Image Registry verwendet werden.

\section{Service-to-Service Kommunikation}
\label{Konzeption_S2S_Kommunikation}
Bei der erstmaligen Konzeption der Portierung der \ac{SaaS}-Lösung SAP Subscription Billing auf das Kubernetes Cluster sollten die nativen \textbf{Service}-Objekte als Grundlage für die asynchrone Kommunikation der Microservices untereinander verwendet werden. Hierbei muss für jeden Microservice ein eigener Service angelegt werden, welcher mittels eines übereinstimmenden \textbf{Labels} dem Container eines Pods zugeordnet wird. Dabei soll ein Microservice Anfragen, welche auf dem \ac{HTTP}-Protokoll basieren, an einen sich im gleichen Namespace lokalisierten Service senden. Dieser leitet die Anfragen automatisch an den im Service hinterlegten Pod weiter. Diese Zuordnung erfolgt über einen Labelselektor im Service selbst, welcher die Anfragen auf alle bereitgestellten Pods mit einem übereinstimmenden Label weiterleitet. Zudem findet hierbei ein automatischer Lastenausgleich der Pods statt. Des Weiteren können neben dem \ac{HTTP}-Protokoll auch weitere Protokolle, wie beispielsweise das \ac{TCP} oder das \ac{UDP}, eingesetzt werden.\autocite[Vgl.][Supported protocols]{KubernetesAuthors.20200115}\\
\\
Während der praktischen Umsetzung stellte sich jedoch heraus, dass SAP interne Sicherheitsvorschriften eine gesicherte Kommunikation der Microservices untereinander vorsehen. Dabei ist zu erwähnen, dass die Kommunikation der Microservices und deren Endpunkte ausschließlich im virtuellen Netzwerk innerhalb des Kubernetes Clusters verfügbar sind.\\
Da im Rahmen der vorliegenden Thesis versucht werden sollte, möglichst viele der aktuell zusätzlichen Infrastruktur-Services durch native Kubernetes-Objekte und Funktionalitäten zu ersetzen, wurde bewusst auf die Absicherung der Kommunikation durch die gegenseitige Authentifikation mit eigenen Credentials verzichtet. Damit soll bewiesen werden, dass der zentrale Credential-Store Vault ersetzt werden kann. 
Die Absicherung der Kommunikation soll stattdessen anhand einer \ac{mTLS}-Verschlüsselung umgesetzt werden. Dies ist ein Authentifizierungsschema für das \ac{HTTP}-Protokoll. 
\newpage
Die Besonderheit von \ac{mTLS} ist, dass sich neben dem Server auch der Client selbst mittels seinem Zertifikat authentifizieren muss.\autocite[Vgl.][S. 4-5]{Oiwa.2017}
\\
Für die Verwendung von \ac{mTLS} soll, wie auch für die praktische Umsetzung des mandantenabhängigen Routings der Anfragen, der Service Mesh Istio verwendet werden. Die generelle Funktionsweise und Implementierung von Istio erfolgt in Kapitel \ref{Umsetzung_K8s_Cluster}.\\ 
Eine kompakte Übersicht weiterer zusätzlicher Funktionalitäten von Istio beinhaltet Kapitel \ref{fazit}. Diese werden zwar im Rahmen der prototypischen Umsetzung des Konzeptes nicht alle implementiert, jedoch sind diese langfristig für das SAP Subscription Billing Projekt sehr relevant. Deshalb werden sie in der Evaluation des Prototyps und den möglichen Funktionalitäten berücksichtigt.

\section{Monitoring und Logging}
\label{Konzeption_Monitoring_Logging}
Wie bereits in Kapitel \ref{kapitel_merkmale_vorgehensweise} beschrieben, sollte im Rahmen dieser Thesis die Möglichkeit der Verwendung der bisher im Projekt genutzten Tools für das Monitoring und Sammeln der Logdateien der Softwarelösung überprüft und umgesetzt werden.\\
Wie bereits in Kapitel \ref{bewertung_cf} erläutert, ist Dynatrace eine \ac{SaaS}-Lösung, welche hauptsächlich Funktionalitäten zur Überwachung der Verfügbarkeit, Performance, sowie Ressourcenauslastung der Anwendungen und auch der Infrastruktur abdeckt. KDabei unterstützt Dynatrace sowohl die \ac{SCP} und \ac{CF} als auch Kubernetes. Dies bietet den großen Vorteil, dass das Tool alle Rechenressourcen und bereitgestellten Anwendungen automatisch erkennt und keine weitere Konfiguration benötigt wird. Zudem werden auch die vom Service Mesh Istio implementierten Objekte automatisch erkannt und können ohne weitere Konfigurationen mit den Funktionalitäten von Dynatrace überwacht werden.\autocite[Vgl.][]{DynatraceLLC.2019}
\\
Bei der Konzeption des Kubernetes Clusters wurde entschieden, dass für das zentrale Speichern der Logdateien der ELK-Stack, dessen Komponenten bereits in Kapitel \ref{bewertung_cf} erklärt wurden, eingesetzt werden soll. Dieser wird ebenfalls bei der aktuellen \ac{CF}-Lösung verwendet und soll auch bei der Verwendung von Kubernetes weiterhin eingesetzt werden. Jedoch wird anstelle von Logstash das Tool \textbf{Filebeat} verwendet werden. Dieses dient, wie auch Logstash, als zentrale Datenverarbeitungspipeline, welche die Logdaten innerhalb des Kubernetes Clusters extrahiert und an den zentralen Speicherort des Elasticsearch-Tools sendet. Das Tools Filebeat wurde ausgewählt, da es die für den Prototyp relevante Grundfunktionalitäten von Logstash abdeckt und generell weniger Rechenressourcen als Logstash benötigt.\footnote{Vergleich zwischen Logstash und Filebeat: \url{https://logz.io/blog/filebeat-vs-logstash/}}\\ 
\\
Die Implementierung der einzelnen Komponenten Elasticsearch, Filestash und Kibana soll mit Hilfe des Open Source Kubernetes Package Mangers \textbf{Helm}\footnote{Helm GitHub Projekt: \url{https://github.com/helm/helm}} durchgeführt werden. Das Kommandozeilentool Helm ermöglicht die Installation vorkonfigurierten Anwendungen und Tools, welche mittels sogenannter \textbf{Charts} in einem zentralen \textbf{Repository} angeboten werden.\autocite[Vgl.][]{HelmAuthors.2020} 
Der Vorteil bei der Verwendung von Helm sind die vorkonfigurierten Anwendungen, wie beispielsweise die einzelnen Komponenten des ELK-Stacks, welche mit Hilfe der Helm \ac{CLI} innerhalb von kurzer Zeit und ohne umfangreiche Konfigurationen in ein Kubernetes Cluster installiert und betrieben werden können.\footnote{Weitere Informationen zu Helm: \url{https://helm.sh/docs/glossary/}}
